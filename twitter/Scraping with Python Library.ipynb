{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367dacd8",
   "metadata": {
    "id": "second-phrase"
   },
   "source": [
    "# Getting Information from Social Media (Twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4d261",
   "metadata": {
    "id": "proof-stand"
   },
   "source": [
    "<img src=\"Images/pic1.JPG\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5325f",
   "metadata": {
    "id": "sexual-vault"
   },
   "source": [
    "+ **Web Crawling** merupakan suatu program/sistem/script otomatis yang dengan suatu metode tertentu melakukan scanning data-data yang ada dalam sebuah website.  \n",
    "+ **Web Scraping** merupakan suatu kegiatan yang dilakukan untuk mengambil informasi dari halaman website. Web scraping biasanya mengambil informasi dari HTML yang terdapat pada halaman website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b1c0d",
   "metadata": {
    "id": "prospective-orchestra"
   },
   "source": [
    "### Contoh Scraper/crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b1c0d5",
   "metadata": {
    "id": "velvet-titanium"
   },
   "source": [
    "**Official Scraper/Crawler**   : Tweepy, Scrapy\n",
    "\n",
    "**Unofficial Scraper/Crawler** : Twitterscraper, Scweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da304404",
   "metadata": {
    "id": "driving-distinction"
   },
   "source": [
    "### Example Scraping\n",
    "\n",
    "Pada sesi ini, kita akan menggunakan salah satu contoh web scraper yaitu Scweet. Scweet melakukan scraping pada halaman website twitter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b4690",
   "metadata": {
    "id": "split-algeria"
   },
   "source": [
    "#### 1. Import needed library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34296299",
   "metadata": {
    "id": "connected-carbon"
   },
   "source": [
    "Library yang dibutuhkan untuk scraping kali ini adalah scweet dan pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68145a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9032,
     "status": "ok",
     "timestamp": 1620128986898,
     "user": {
      "displayName": "Naufal Dzaky Anwari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjUfVofiHSrNTGJZHJIPJwL0sf7lvJPIrE2d4XMAw=s64",
      "userId": "06742799176171152084"
     },
     "user_tz": -420
    },
    "id": "lI1hgalzNgvm",
    "outputId": "98c3da84-fb57-4edb-a9d0-73616e2e2579"
   },
   "outputs": [],
   "source": [
    "!pip install Scweet==1.0\n",
    "!pip install pandas==1.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6206f9b9",
   "metadata": {
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1620128992832,
     "user": {
      "displayName": "Naufal Dzaky Anwari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjUfVofiHSrNTGJZHJIPJwL0sf7lvJPIrE2d4XMAw=s64",
      "userId": "06742799176171152084"
     },
     "user_tz": -420
    },
    "id": "unlike-minneapolis"
   },
   "outputs": [],
   "source": [
    "from Scweet.scweet import scrap\n",
    "from Scweet.user import get_user_information, get_users_following, get_users_followers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865ce624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09bf90",
   "metadata": {
    "id": "random-element"
   },
   "source": [
    "#### 2. Scrape tweet with certain words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512ec5b",
   "metadata": {
    "id": "convinced-miami"
   },
   "source": [
    "dengan menggunakan Scweet, kita dapat mengambil top tweets yang mengandung kata tertentu. caranya dengan menggunakan module scrap yang tersedia pada library Scweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e83caee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 2056,
     "status": "error",
     "timestamp": 1620128998741,
     "user": {
      "displayName": "Naufal Dzaky Anwari",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjUfVofiHSrNTGJZHJIPJwL0sf7lvJPIrE2d4XMAw=s64",
      "userId": "06742799176171152084"
     },
     "user_tz": -420
    },
    "id": "strong-relief",
    "outputId": "23b35e02-150f-45c0-b71b-737a5c3def31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping on headless mode.\n",
      "looking for tweets between 2021-04-20 and 2021-04-21 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-21%20since%3A2021-04-20%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "Tweet made at: 2021-04-20T16:55:47.000Z is found.\n",
      "scroll  3\n",
      "scroll  4\n",
      "looking for tweets between 2021-04-21 and 2021-04-22 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-22%20since%3A2021-04-21%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-22 and 2021-04-23 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-23%20since%3A2021-04-22%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-23 and 2021-04-24 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-24%20since%3A2021-04-23%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-24 and 2021-04-25 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-25%20since%3A2021-04-24%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "Tweet made at: 2021-04-24T05:52:15.000Z is found.\n",
      "scroll  1\n",
      "scroll  2\n",
      "scroll  3\n",
      "looking for tweets between 2021-04-25 and 2021-04-26 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-26%20since%3A2021-04-25%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-26 and 2021-04-27 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-27%20since%3A2021-04-26%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-27 and 2021-04-28 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-28%20since%3A2021-04-27%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-28 and 2021-04-29 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-29%20since%3A2021-04-28%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n",
      "looking for tweets between 2021-04-29 and 2021-04-30 ...\n",
      " path : https://twitter.com/search?q=(kolak)%20until%3A2021-04-30%20since%3A2021-04-29%20%20-filter%3Areplies&src=typed_query&lf=on\n",
      "scroll  1\n",
      "scroll  2\n"
     ]
    }
   ],
   "source": [
    "# keywords\n",
    "keywords = ['kolak']\n",
    "\n",
    "# Date interval\n",
    "initial_date = '2021-04-20'\n",
    "finish_date = '2021-04-30'\n",
    "\n",
    "all_datas = []\n",
    "for x in keywords:\n",
    "    data = scrap(words=x,\n",
    "                 start_date=initial_date, #penting\n",
    "                 max_date=finish_date, #penting\n",
    "                 from_account=None, #bisa spesific account\n",
    "                 interval=1, \n",
    "                 headless=True,\n",
    "                 save_images=False,\n",
    "                 display_type=None,\n",
    "                 resume=False,\n",
    "                 filter_replies=True,\n",
    "                 proximity=True)\n",
    "    \n",
    "    data['keyword'] = x\n",
    "    all_datas.append(data)\n",
    "\n",
    "all_datas = pd.concat(all_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c245267d",
   "metadata": {
    "id": "polished-jenny",
    "outputId": "c22488d5-c278-4d57-a165-e3f2f90a688e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserScreenName</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedded_text</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Image link</th>\n",
       "      <th>Tweet URL</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brunns</td>\n",
       "      <td>@fikrialhakimm</td>\n",
       "      <td>2021-04-20T16:55:47.000Z</td>\n",
       "      <td>Bisa bisanya manusia makan kolak durian, aneh</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/fikrialhakimm/status/13845...</td>\n",
       "      <td>kolak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waroeng Snoepen</td>\n",
       "      <td>@wsnoepen</td>\n",
       "      <td>2021-04-24T05:52:15.000Z</td>\n",
       "      <td>Selamat siang, 12:47 WIB dan sudah mulai bosen...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>[https://pbs.twimg.com/media/Ezt4n7_VkAA_pcd?f...</td>\n",
       "      <td>https://twitter.com/wsnoepen/status/1385833948...</td>\n",
       "      <td>kolak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UserScreenName        UserName                 Timestamp  \\\n",
       "0           brunns  @fikrialhakimm  2021-04-20T16:55:47.000Z   \n",
       "1  Waroeng Snoepen       @wsnoepen  2021-04-24T05:52:15.000Z   \n",
       "\n",
       "                                                Text Embedded_text Emojis  \\\n",
       "0      Bisa bisanya manusia makan kolak durian, aneh                        \n",
       "1  Selamat siang, 12:47 WIB dan sudah mulai bosen...                        \n",
       "\n",
       "  Comments Likes Retweets                                         Image link  \\\n",
       "0                                                                         []   \n",
       "1              2           [https://pbs.twimg.com/media/Ezt4n7_VkAA_pcd?f...   \n",
       "\n",
       "                                           Tweet URL keyword  \n",
       "0  https://twitter.com/fikrialhakimm/status/13845...   kolak  \n",
       "1  https://twitter.com/wsnoepen/status/1385833948...   kolak  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57191472",
   "metadata": {
    "id": "lightweight-surfing"
   },
   "outputs": [],
   "source": [
    "# Save data to csv\n",
    "filename = 'Data/all_keywordsv2.csv'\n",
    "all_datas.to_csv(filename, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c228e4",
   "metadata": {
    "id": "incorporated-chemical",
    "outputId": "670abc25-c7e3-448d-86bb-7edaeaf28648"
   },
   "outputs": [],
   "source": [
    "hashtag = 'kolak'\n",
    "\n",
    "initial_date = '2021-04-28'\n",
    "finish_date = '2021-04-30'\n",
    "\n",
    "data = scrap(hashtag=hashtag,\n",
    "             start_date=initial_date,\n",
    "             max_date=finish_date,\n",
    "             from_account=None,\n",
    "             interval=5,\n",
    "             headless=True,\n",
    "             display_type=\"Top\",\n",
    "             save_images=False, \n",
    "             resume=False,\n",
    "             filter_replies=False,\n",
    "             proximity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8368c",
   "metadata": {
    "id": "norman-hospital",
    "outputId": "ef61011f-67d1-41c9-9702-1f5d5434490e"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e88064",
   "metadata": {
    "id": "swiss-spanking"
   },
   "source": [
    "### Get the main information of a given list of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c038b11e",
   "metadata": {
    "id": "amazing-bottom",
    "outputId": "89313c32-04f6-4f2a-be7e-b494ad4a80b7"
   },
   "outputs": [],
   "source": [
    "users = ['@raisa6690', '@isyanasarasvati']\n",
    "\n",
    "# this function return a list that contains : \n",
    "# [\"nb of following\",\"nb of followers\", \"join date\", \"birthdate\", \"location\", \"website\", \"description\"]\n",
    "\n",
    "users_info = get_user_information(users, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecbbf9c",
   "metadata": {
    "id": "novel-service",
    "outputId": "1cf729d8-1329-41a7-fbb5-9f2457f18e37"
   },
   "outputs": [],
   "source": [
    "users_df = pd.DataFrame(users_info, index = [\"nb of following\",\n",
    "                                             \"nb of followers\",\n",
    "                                             \"join date\", \n",
    "                                             \"birthdate\",\n",
    "                                             \"location\",\n",
    "                                             \"website\",\n",
    "                                             \"description\"]).T\n",
    "users_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Scraping with Python Library.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
